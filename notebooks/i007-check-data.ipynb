{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=df('TestPubMedCitation2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMID:String                 = 14934087_1\n",
      "Journal_ISSN:String         = Print_0003-5998\n",
      "Article_Title:String        = Filariasis of the scrotum.\n",
      "Journal_Title:String        = The Antiseptic\n",
      "Journal_Publish_Date:String = 1952-05-01\n",
      "Journal_Country:String      = Not Available\n",
      "Mesh_Headings:String        = D004194|D005368|D005832|D006801|D008297|D012611\n",
      "Keywords:String             = FILARIASIS|SCROTUM/diseases\n",
      "Abstract:String             = null\n",
      "Affiliation:String          = null\n",
      "Author_Identifier:String    = null\n",
      "LastName:String             = SHAH\n",
      "ForeName:String             = K S\n",
      "Suffix:String               = null\n",
      "Initials:String             = KS\n"
     ]
    }
   ],
   "source": [
    "a.peek()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMID:String                 = 17749162_1\n",
      "Journal_ISSN:String         = Print_0036-8075\n",
      "Article_Title:String        = The Isotope Distribution Program.\n",
      "Journal_Title:String        = Science (New York, N.Y.)\n",
      "Journal_Publish_Date:String = 1947-08-29\n",
      "Journal_Country:String      = United States\n",
      "Mesh_Headings:String        = D006801|D007554\n",
      "Keywords:String             = ISOTOPES\n",
      "Abstract:String             = null\n",
      "Affiliation:String          = null\n",
      "Author_Identifier:String    = null\n",
      "LastName:String             = null\n",
      "ForeName:String             = null\n",
      "Suffix:String               = null\n",
      "Initials:String             = null\n"
     ]
    }
   ],
   "source": [
    "a.where(F.col('LastName').isNull()).peek()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are records with Author with `CollectiveName` only:\n",
    "```\n",
    "<Author ValidYN=\"Y\">\n",
    "<CollectiveName>Isotopes Branch, U. S. Atomic Energy Commission</CollectiveName>\n",
    "</Author>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d                    Time Start             \"1809-01-01 00:00:00\"\n",
      "d                    Time Edd               \"1959-11-21 00:00:00\"\n",
      "Histogram of d: Year\n",
      "key                      count      Pct    cumCount   cumPct\n",
      "1809                        20    0.00%          20    0.00%\n",
      "1811                        27    0.00%          47    0.00%\n",
      "1812                        21    0.00%          68    0.00%\n",
      "1813                        23    0.00%          91    0.00%\n",
      "1814                        27    0.00%         118    0.01%\n",
      "1815                        27    0.00%         145    0.01%\n",
      "1816                        39    0.00%         184    0.01%\n",
      "1817                        21    0.00%         205    0.01%\n",
      "1818                        29    0.00%         234    0.01%\n",
      "1819                        28    0.00%         262    0.01%\n",
      "1821                        30    0.00%         292    0.01%\n",
      "1823                        46    0.00%         338    0.02%\n",
      "1827                        44    0.00%         382    0.02%\n",
      "1828                        12    0.00%         394    0.02%\n",
      "1829                        10    0.00%         404    0.02%\n",
      "1831                        21    0.00%         425    0.02%\n",
      "1832                        16    0.00%         441    0.02%\n",
      "1833                        13    0.00%         454    0.02%\n",
      "1835                        20    0.00%         474    0.02%\n",
      "1837                        17    0.00%         491    0.02%\n",
      "1838                        25    0.00%         516    0.02%\n",
      "1839                        24    0.00%         540    0.03%\n",
      "1840                       100    0.00%         640    0.03%\n",
      "1841                       318    0.01%         958    0.04%\n",
      "1842                       384    0.02%        1342    0.06%\n",
      "1843                       314    0.01%        1656    0.08%\n",
      "1844                       217    0.01%        1873    0.09%\n",
      "1845                       182    0.01%        2055    0.10%\n",
      "1846                       149    0.01%        2204    0.10%\n",
      "1847                       140    0.01%        2344    0.11%\n",
      "1848                       127    0.01%        2471    0.12%\n",
      "1849                       104    0.00%        2575    0.12%\n",
      "1850                       113    0.01%        2688    0.13%\n",
      "1851                       131    0.01%        2819    0.13%\n",
      "1852                       145    0.01%        2964    0.14%\n",
      "1853                       120    0.01%        3084    0.14%\n",
      "1854                        16    0.00%        3100    0.14%\n",
      "1855                        48    0.00%        3148    0.15%\n",
      "1856                       128    0.01%        3276    0.15%\n",
      "1857                        55    0.00%        3331    0.16%\n",
      "1858                        51    0.00%        3382    0.16%\n",
      "1859                        70    0.00%        3452    0.16%\n",
      "1860                        95    0.00%        3547    0.17%\n",
      "1861                        89    0.00%        3636    0.17%\n",
      "1862                        93    0.00%        3729    0.17%\n",
      "1863                        86    0.00%        3815    0.18%\n",
      "1864                        84    0.00%        3899    0.18%\n",
      "1865                        63    0.00%        3962    0.18%\n",
      "1866                        73    0.00%        4035    0.19%\n",
      "1867                       272    0.01%        4307    0.20%\n",
      "1868                       358    0.02%        4665    0.22%\n",
      "1869                       291    0.01%        4956    0.23%\n",
      "1870                       302    0.01%        5258    0.25%\n",
      "1871                       357    0.02%        5615    0.26%\n",
      "1872                       394    0.02%        6009    0.28%\n",
      "1873                       443    0.02%        6452    0.30%\n",
      "1874                       363    0.02%        6815    0.32%\n",
      "1875                       395    0.02%        7210    0.34%\n",
      "1876                       408    0.02%        7618    0.36%\n",
      "1877                       372    0.02%        7990    0.37%\n",
      "1878                       479    0.02%        8469    0.40%\n",
      "1879                       460    0.02%        8929    0.42%\n",
      "1880                       639    0.03%        9568    0.45%\n",
      "1881                       714    0.03%       10282    0.48%\n",
      "1882                       423    0.02%       10705    0.50%\n",
      "1883                       814    0.04%       11519    0.54%\n",
      "1884                       918    0.04%       12437    0.58%\n",
      "1885                       931    0.04%       13368    0.62%\n",
      "1886                      1097    0.05%       14465    0.67%\n",
      "1887                      1047    0.05%       15512    0.72%\n",
      "1888                       809    0.04%       16321    0.76%\n",
      "1889                       696    0.03%       17017    0.79%\n",
      "1890                       712    0.03%       17729    0.83%\n",
      "1891                       690    0.03%       18419    0.86%\n",
      "1892                      1066    0.05%       19485    0.91%\n",
      "1893                      1229    0.06%       20714    0.97%\n",
      "1894                       733    0.03%       21447    1.00%\n",
      "1895                      1058    0.05%       22505    1.05%\n",
      "1896                      1252    0.06%       23757    1.11%\n",
      "1897                      1376    0.06%       25133    1.17%\n",
      "1898                      1239    0.06%       26372    1.23%\n",
      "1899                      1115    0.05%       27487    1.28%\n",
      "1900                      1118    0.05%       28605    1.33%\n",
      "1901                      1370    0.06%       29975    1.40%\n",
      "1902                      1328    0.06%       31303    1.46%\n",
      "1903                      1428    0.07%       32731    1.53%\n",
      "1904                      1380    0.06%       34111    1.59%\n",
      "1905                      1636    0.08%       35747    1.67%\n",
      "1906                      1617    0.08%       37364    1.74%\n",
      "1907                      1637    0.08%       39001    1.82%\n",
      "1908                      2257    0.11%       41258    1.92%\n",
      "1909                      2611    0.12%       43869    2.05%\n",
      "1910                      2583    0.12%       46452    2.17%\n",
      "1911                      2878    0.13%       49330    2.30%\n",
      "1912                      2981    0.14%       52311    2.44%\n",
      "1913                      3244    0.15%       55555    2.59%\n",
      "1914                      3081    0.14%       58636    2.74%\n",
      "1915                      3016    0.14%       61652    2.88%\n",
      "1916                      2822    0.13%       64474    3.01%\n",
      "1917                      2764    0.13%       67238    3.14%\n",
      "1918                      2693    0.13%       69931    3.26%\n",
      "1919                      2922    0.14%       72853    3.40%\n",
      "1920                      3185    0.15%       76038    3.55%\n",
      "1921                      3326    0.16%       79364    3.70%\n",
      "1922                      3491    0.16%       82855    3.87%\n",
      "1923                      3706    0.17%       86561    4.04%\n",
      "1924                      4085    0.19%       90646    4.23%\n",
      "1925                      4142    0.19%       94788    4.42%\n",
      "1926                      4392    0.20%       99180    4.63%\n",
      "1927                      4939    0.23%      104119    4.86%\n",
      "1928                      4937    0.23%      109056    5.09%\n",
      "1929                      4677    0.22%      113733    5.31%\n",
      "1930                      5123    0.24%      118856    5.54%\n",
      "1931                      5136    0.24%      123992    5.78%\n",
      "1932                      5308    0.25%      129300    6.03%\n",
      "1933                      5243    0.24%      134543    6.28%\n",
      "1934                      5305    0.25%      139848    6.52%\n",
      "1935                      5549    0.26%      145397    6.78%\n",
      "1936                      5383    0.25%      150780    7.03%\n",
      "1937                      5357    0.25%      156137    7.28%\n",
      "1938                      5638    0.26%      161775    7.55%\n",
      "1939                      5694    0.27%      167469    7.81%\n",
      "1940                      5224    0.24%      172693    8.06%\n",
      "1941                      5070    0.24%      177763    8.29%\n",
      "1942                      5069    0.24%      182832    8.53%\n",
      "1943                      5023    0.23%      187855    8.76%\n",
      "1944                      5461    0.25%      193316    9.02%\n",
      "1945                     27057    1.26%      220373   10.28%\n",
      "1946                     72171    3.37%      292544   13.65%\n",
      "1947                     88492    4.13%      381036   17.78%\n",
      "1948                     98212    4.58%      479248   22.36%\n",
      "1949                     89707    4.18%      568955   26.54%\n",
      "1950                    126201    5.89%      695156   32.43%\n",
      "1951                    152199    7.10%      847355   39.53%\n",
      "1952                    157682    7.36%     1005037   46.88%\n",
      "1953                    164992    7.70%     1170029   54.58%\n",
      "1954                    168615    7.87%     1338644   62.45%\n",
      "1955                    175155    8.17%     1513799   70.62%\n",
      "1956                    177808    8.29%     1691607   78.91%\n",
      "1957                    188669    8.80%     1880276   87.71%\n",
      "1958                    186270    8.69%     2066546   96.40%\n",
      "1959                     77104    3.60%     2143650  100.00%\n",
      "-------------------------------------------------\n",
      "Histogram of d: Month\n",
      "key                      count      Pct    cumCount   cumPct\n",
      "01                      469719   21.91%      469719   21.91%\n",
      "02                      133373    6.22%      603092   28.13%\n",
      "03                      173441    8.09%      776533   36.22%\n",
      "04                      158818    7.41%      935351   43.63%\n",
      "05                      169448    7.90%     1104799   51.54%\n",
      "06                      145192    6.77%     1249991   58.31%\n",
      "07                      173564    8.10%     1423555   66.41%\n",
      "08                      122139    5.70%     1545694   72.11%\n",
      "09                      151314    7.06%     1697008   79.16%\n",
      "10                      151537    7.07%     1848545   86.23%\n",
      "11                      153604    7.17%     2002149   93.40%\n",
      "12                      141501    6.60%     2143650  100.00%\n",
      "-------------------------------------------------\n",
      "Histogram of d: Day of Week\n",
      "key                      count      Pct    cumCount   cumPct\n",
      "1                       266046   12.41%      266046   12.41%\n",
      "2                       287866   13.43%      553912   25.84%\n",
      "3                       257262   12.00%      811174   37.84%\n",
      "4                       299767   13.98%     1110941   51.82%\n",
      "5                       346649   16.17%     1457590   68.00%\n",
      "6                       408239   19.04%     1865829   87.04%\n",
      "7                       277821   12.96%     2143650  100.00%\n",
      "-------------------------------------------------\n",
      "Histogram of d: Hour\n",
      "key                      count      Pct    cumCount   cumPct\n",
      "00                     2143650  100.00%     2143650  100.00%\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "a.select(F.col('Journal_Publish_Date').smvStrToTimestamp('yyyy-MM-dd').alias('d')).smvEdd('d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMID                 Non-Null Count         2143650\n",
      "PMID                 Null Count             0\n",
      "PMID                 Min Length             10\n",
      "PMID                 Max Length             10\n",
      "PMID                 Approx Distinct Count  1365909\n",
      "Journal_ISSN         Non-Null Count         1941168\n",
      "Journal_ISSN         Null Count             202482\n",
      "Journal_ISSN         Min Length             15\n",
      "Journal_ISSN         Max Length             20\n",
      "Journal_ISSN         Approx Distinct Count  2427\n",
      "Article_Title        Non-Null Count         2143650\n",
      "Article_Title        Null Count             0\n",
      "Article_Title        Min Length             4\n",
      "Article_Title        Max Length             502\n",
      "Article_Title        Approx Distinct Count  1208432\n",
      "Journal_Title        Non-Null Count         2143650\n",
      "Journal_Title        Null Count             0\n",
      "Journal_Title        Min Length             2\n",
      "Journal_Title        Max Length             275\n",
      "Journal_Title        Approx Distinct Count  3563\n",
      "Journal_Publish_Date Non-Null Count         2143650\n",
      "Journal_Publish_Date Null Count             0\n",
      "Journal_Publish_Date Min Length             10\n",
      "Journal_Publish_Date Max Length             10\n",
      "Journal_Publish_Date Approx Distinct Count  13982\n",
      "Journal_Country      Non-Null Count         2143650\n",
      "Journal_Country      Null Count             0\n",
      "Journal_Country      Min Length             5\n",
      "Journal_Country      Max Length             16\n",
      "Journal_Country      Approx Distinct Count  14\n",
      "Mesh_Headings        Non-Null Count         1922033\n",
      "Mesh_Headings        Null Count             221617\n",
      "Mesh_Headings        Min Length             7\n",
      "Mesh_Headings        Max Length             895\n",
      "Mesh_Headings        Approx Distinct Count  790540\n",
      "Keywords             Non-Null Count         1902455\n",
      "Keywords             Null Count             241195\n",
      "Keywords             Min Length             3\n",
      "Keywords             Max Length             1096\n",
      "Keywords             Approx Distinct Count  502249\n",
      "Abstract             Non-Null Count         20251\n",
      "Abstract             Null Count             2123399\n",
      "Abstract             Min Length             62\n",
      "Abstract             Max Length             9969\n",
      "Abstract             Approx Distinct Count  11437\n",
      "Affiliation          Non-Null Count         36304\n",
      "Affiliation          Null Count             2107346\n",
      "Affiliation          Min Length             4\n",
      "Affiliation          Max Length             389\n",
      "Affiliation          Approx Distinct Count  14974\n",
      "Author_Identifier    Non-Null Count         0\n",
      "Author_Identifier    Null Count             2143650\n",
      "Author_Identifier    Min Length             null\n",
      "Author_Identifier    Max Length             null\n",
      "Author_Identifier    Approx Distinct Count  0\n",
      "LastName             Non-Null Count         2143370\n",
      "LastName             Null Count             280\n",
      "LastName             Min Length             1\n",
      "LastName             Max Length             42\n",
      "LastName             Approx Distinct Count  212992\n",
      "ForeName             Non-Null Count         2106336\n",
      "ForeName             Null Count             37314\n",
      "ForeName             Min Length             1\n",
      "ForeName             Max Length             13\n",
      "ForeName             Approx Distinct Count  5307\n",
      "Suffix               Non-Null Count         28372\n",
      "Suffix               Null Count             2115278\n",
      "Suffix               Min Length             2\n",
      "Suffix               Max Length             3\n",
      "Suffix               Approx Distinct Count  6\n",
      "Initials             Non-Null Count         2106336\n",
      "Initials             Null Count             37314\n",
      "Initials             Min Length             1\n",
      "Initials             Max Length             6\n",
      "Initials             Approx Distinct Count  1145\n"
     ]
    }
   ],
   "source": [
    "a.smvEdd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Since the partition files are time ordered, need to check the lastest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o23.runModuleByName.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 281.0 failed 1 times, most recent failure: Lost task 2.0 in stage 281.0 (TID 628, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/bzhang/spark-bin/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/bzhang/spark-bin/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/bzhang/spark-bin/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/bzhang/spark-bin/python/pyspark/sql/functions.py\", line 1566, in <lambda>\n    func = lambda _, it: map(lambda x: returnType.toInternal(f(*x)), it)\n  File \"/home/bzhang/DS/PubMedReader/src/main/python/pubmed/core.py\", line 41, in <lambda>\n    _udf = lambda c: \"|\".join([e for e in c]) if isinstance(c, list) else c\nTypeError: sequence item 1: expected string or Unicode, NoneType found\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:129)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:125)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:913)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:929)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:969)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:972)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1817)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1922)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1209)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1154)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1154)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1154)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1060)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:951)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1457)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1436)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1436)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1436)\n\tat org.tresamigos.smv.FileIOHandler.saveAsCsvWithSchema(FileIOHandler.scala:158)\n\tat org.tresamigos.smv.SmvDataSet.persist(SmvDataSet.scala:308)\n\tat org.tresamigos.smv.SmvDataSet$$anonfun$computeRDD$1$$anonfun$applyOrElse$1$$anonfun$apply$1.applyOrElse(SmvDataSet.scala:495)\n\tat org.tresamigos.smv.SmvDataSet$$anonfun$computeRDD$1$$anonfun$applyOrElse$1$$anonfun$apply$1.applyOrElse(SmvDataSet.scala:491)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)\n\tat scala.util.Failure.recoverWith(Try.scala:172)\n\tat org.tresamigos.smv.SmvDataSet$$anonfun$computeRDD$1$$anonfun$applyOrElse$1.apply(SmvDataSet.scala:491)\n\tat org.tresamigos.smv.SmvDataSet$$anonfun$computeRDD$1$$anonfun$applyOrElse$1.apply(SmvDataSet.scala:491)\n\tat org.tresamigos.smv.SmvLock$.withLock(SmvLock.scala:65)\n\tat org.tresamigos.smv.SmvDataSet$$anonfun$computeRDD$1.applyOrElse(SmvDataSet.scala:488)\n\tat org.tresamigos.smv.SmvDataSet$$anonfun$computeRDD$1.applyOrElse(SmvDataSet.scala:486)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)\n\tat scala.util.Failure.recoverWith(Try.scala:172)\n\tat org.tresamigos.smv.SmvDataSet.computeRDD(SmvDataSet.scala:486)\n\tat org.tresamigos.smv.SmvDataSet.rdd(SmvDataSet.scala:209)\n\tat org.tresamigos.smv.SmvApp.runDS(SmvApp.scala:343)\n\tat org.tresamigos.smv.SmvApp.runModuleByName(SmvApp.scala:374)\n\tat org.tresamigos.smv.python.SmvPyClient.runModuleByName(SmvPythonProxy.scala:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/bzhang/spark-bin/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/bzhang/spark-bin/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/bzhang/spark-bin/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/bzhang/spark-bin/python/pyspark/sql/functions.py\", line 1566, in <lambda>\n    func = lambda _, it: map(lambda x: returnType.toInternal(f(*x)), it)\n  File \"/home/bzhang/DS/PubMedReader/src/main/python/pubmed/core.py\", line 41, in <lambda>\n    _udf = lambda c: \"|\".join([e for e in c]) if isinstance(c, list) else c\nTypeError: sequence item 1: expected string or Unicode, NoneType found\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:129)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:125)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:913)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:929)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:969)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:972)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1817)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-77fc5010cf1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PubMedCitation8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/bzhang/Tresamigos/SMV/src/main/python/smv/smvshell.py\u001b[0m in \u001b[0;36mdf\u001b[0;34m(name, forceRun, version, runConfig)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mresult\u001b[0m \u001b[0mof\u001b[0m \u001b[0mrunning\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnamed\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \"\"\"\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mSmvApp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetInstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunModuleByName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforceRun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdshash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bzhang/Tresamigos/SMV/src/main/python/smv/smvapp.py\u001b[0m in \u001b[0;36mrunModuleByName\u001b[0;34m(self, name, forceRun, version, runConfig)\u001b[0m\n\u001b[1;32m    211\u001b[0m               \u001b[0mabout\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuch\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mvalidation\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \"\"\"\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mjava_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mj_smvPyClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunModuleByName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforceRun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalaOption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         return (DataFrame(java_result.df(), self.sqlContext),\n\u001b[1;32m    215\u001b[0m                 SmvRunInfoCollector(java_result.collector()) )\n",
      "\u001b[0;32m/home/bzhang/spark-bin/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 813\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bzhang/spark-bin/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/bzhang/spark-bin/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    307\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    309\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o23.runModuleByName.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 281.0 failed 1 times, most recent failure: Lost task 2.0 in stage 281.0 (TID 628, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/bzhang/spark-bin/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/bzhang/spark-bin/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/bzhang/spark-bin/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/bzhang/spark-bin/python/pyspark/sql/functions.py\", line 1566, in <lambda>\n    func = lambda _, it: map(lambda x: returnType.toInternal(f(*x)), it)\n  File \"/home/bzhang/DS/PubMedReader/src/main/python/pubmed/core.py\", line 41, in <lambda>\n    _udf = lambda c: \"|\".join([e for e in c]) if isinstance(c, list) else c\nTypeError: sequence item 1: expected string or Unicode, NoneType found\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:129)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:125)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:913)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:929)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:969)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:972)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1817)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1922)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1209)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1154)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1154)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1154)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1060)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:951)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1457)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1436)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1436)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1436)\n\tat org.tresamigos.smv.FileIOHandler.saveAsCsvWithSchema(FileIOHandler.scala:158)\n\tat org.tresamigos.smv.SmvDataSet.persist(SmvDataSet.scala:308)\n\tat org.tresamigos.smv.SmvDataSet$$anonfun$computeRDD$1$$anonfun$applyOrElse$1$$anonfun$apply$1.applyOrElse(SmvDataSet.scala:495)\n\tat org.tresamigos.smv.SmvDataSet$$anonfun$computeRDD$1$$anonfun$applyOrElse$1$$anonfun$apply$1.applyOrElse(SmvDataSet.scala:491)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)\n\tat scala.util.Failure.recoverWith(Try.scala:172)\n\tat org.tresamigos.smv.SmvDataSet$$anonfun$computeRDD$1$$anonfun$applyOrElse$1.apply(SmvDataSet.scala:491)\n\tat org.tresamigos.smv.SmvDataSet$$anonfun$computeRDD$1$$anonfun$applyOrElse$1.apply(SmvDataSet.scala:491)\n\tat org.tresamigos.smv.SmvLock$.withLock(SmvLock.scala:65)\n\tat org.tresamigos.smv.SmvDataSet$$anonfun$computeRDD$1.applyOrElse(SmvDataSet.scala:488)\n\tat org.tresamigos.smv.SmvDataSet$$anonfun$computeRDD$1.applyOrElse(SmvDataSet.scala:486)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)\n\tat scala.util.Failure.recoverWith(Try.scala:172)\n\tat org.tresamigos.smv.SmvDataSet.computeRDD(SmvDataSet.scala:486)\n\tat org.tresamigos.smv.SmvDataSet.rdd(SmvDataSet.scala:209)\n\tat org.tresamigos.smv.SmvApp.runDS(SmvApp.scala:343)\n\tat org.tresamigos.smv.SmvApp.runModuleByName(SmvApp.scala:374)\n\tat org.tresamigos.smv.python.SmvPyClient.runModuleByName(SmvPythonProxy.scala:272)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/bzhang/spark-bin/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/bzhang/spark-bin/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/bzhang/spark-bin/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/bzhang/spark-bin/python/pyspark/sql/functions.py\", line 1566, in <lambda>\n    func = lambda _, it: map(lambda x: returnType.toInternal(f(*x)), it)\n  File \"/home/bzhang/DS/PubMedReader/src/main/python/pubmed/core.py\", line 41, in <lambda>\n    _udf = lambda c: \"|\".join([e for e in c]) if isinstance(c, list) else c\nTypeError: sequence item 1: expected string or Unicode, NoneType found\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:129)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:125)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:913)\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:929)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:969)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:972)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:280)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1817)\n\tat org.apache.spark.api.python.PythonRunner$WriterThread.run(PythonRDD.scala:239)\n"
     ]
    }
   ],
   "source": [
    "b=df('PubMedCitation8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
